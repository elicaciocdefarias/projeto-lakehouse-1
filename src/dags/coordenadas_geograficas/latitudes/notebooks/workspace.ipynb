{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198101c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b278a1f",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "minio_connection = \"\"\n",
    "nessie_connection = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427e036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "try:\n",
    "    minio_conn = json.loads(minio_connection)\n",
    "except json.JSONDecodeError:\n",
    "    with open('../local_variables/minio_connection.json', \"r\") as minio_connection_file:\n",
    "        minio_conn = json.loads(minio_connection_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ef2ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "try:\n",
    "    nessie_conn = json.loads(nessie_connection)\n",
    "except json.JSONDecodeError:\n",
    "    with open('../local_variables/nessie_connection.json', \"r\") as nessie_connection_file:\n",
    "        nessie_conn = json.loads(nessie_connection_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a191bf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AWS_REGION\"]=minio_conn.get(\"aws_region\")\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]=minio_conn.get(\"aws_access_key_id\")\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]=minio_conn.get(\"aws_secret_access_key\")\n",
    "# os.environ[\"AWS_S3_ENDPOINT\"] = minio_conn.get(\"aws_s3_endpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12e078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazySparkSession:\n",
    "    packages = [\n",
    "        \"org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.103.6\",\n",
    "        \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.1\",\n",
    "        \"software.amazon.awssdk:bundle:2.31.68\",\n",
    "        \"software.amazon.awssdk:url-connection-client:2.31.68\",\n",
    "    ]\n",
    "\n",
    "    extensions = [\n",
    "        \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "        \"org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n",
    "    ]\n",
    "\n",
    "    def start(\n",
    "        self,\n",
    "        app_name: str = \"Default App Name\",\n",
    "        executor_memory: str = \"1g\",\n",
    "        driver_memory: str = \"1g\",\n",
    "        driver_maxresultsize: str = \"1g\",\n",
    "        master_url: str = \"local[*]\",\n",
    "    ):\n",
    "\n",
    "        spark = (\n",
    "            SparkSession\n",
    "            .Builder()\n",
    "            .appName(app_name)\n",
    "            # Configurações Iceberg e Nessie\n",
    "            .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "            .config(\"spark.sql.catalog.nessie.uri\", nessie_conn.get(\"nessie_uri\"))\n",
    "            .config(\"spark.sql.catalog.nessie.warehouse\", \"s3a://nessie/warehouse/\")\n",
    "            .config(\"spark.sql.catalog.nessie.type\", \"rest\")\n",
    "            # .config(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "            # .config(\"spark.sql.catalog.nessie.s3.endpoint\", minio_conn.get(\"aws_s3_endpoint\"))\n",
    "            # .config(\"spark.sql.catalog.nessie.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "            # .config(\"spark.sql.catalog.nessie.ref\",\"main\")\n",
    "            # .config(\"spark.sql.defaultCatalog\", \"nessie\")\n",
    "            # Configurações Spark\n",
    "            .config(\"spark.executor.memory\", executor_memory)\n",
    "            .config(\"spark.driver.memory\", driver_memory)\n",
    "            .config(\"spark.driver.maxResultSize\", driver_maxresultsize)\n",
    "            # Lista de jars\n",
    "            .config(\"spark.sql.extensions\", \",\".join(self.extensions))\n",
    "            .config(\"spark.jars.packages\", \",\".join(self.packages))\n",
    "            .master(master_url)\n",
    "            .getOrCreate()\n",
    "        )\n",
    "\n",
    "        spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "        return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb9eff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with LazySparkSession().start(app_name=\"Airflow Spark Iceberg Nessie Pipeline\") as spark:\n",
    "    \n",
    "    print(f'The PySpark {spark.version} version is running...')\n",
    "\n",
    "    spark.sql(\"CREATE DATABASE IF NOT EXISTS nessie.coordenadas_geograficas\")\n",
    "\n",
    "    df = spark.createDataFrame(\n",
    "        [\n",
    "            (\"maceio\", -9.66625, -35.7351),\n",
    "            (\"arapiraca\", -9.75164, -36.6604)\n",
    "        ], \n",
    "            schema=\"cidade string,latitude double,longitude double\"\n",
    "    )\n",
    "\n",
    "    df.writeTo(\"coordenadas_geograficas.latitudes\").createOrReplace()\n",
    "    spark.table(\"coordenadas_geograficas.latitudes\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
